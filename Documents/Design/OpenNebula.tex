The OpenNebula backend is meant mainly as a playground until an OpenStack testbed is available. 

The demonstrator will use the lxcloud infrastructure at CERN.
When designing a backend for CloudMan, two different aspects of resource management need to be respected. 
\begin{enumerate}
\item Setting quotas and ACLs for a public cloud infrastructure
\item Moving resources between different projects
\end{enumerate}

The first case only deals with quotas and user groups inside a small OpenNebula driven public cloud. This is the main goal of the prototype driver. 

The second case can only work if all projects share the same resources. This should be supported by the prototype backend for OpenNebula at a basic level. The example use case is a small infrastructure which provides resources for a public cloud interface, and a virtual batch farm.


\subsubsection{The model}
This use case is fairly simple, and only requires the backend to consume data from the Cloudman front end. 
For the prototype we only consider 2 cases: a public batch farm, and a EC2 based self-service.

The model to be implemented looks like this:
\begin{enumerate}
\item The Cloudman adminstrator defines a top level allocation per user group, and defines the administrator egroup for this allocation. This allocation is the total resource allocation (in HS06) for the user community. For CERN, user communities are ATLAS, CMS, SFT, IT, ...
\item The CloudMan administrator defines a new project (eg ONE Public Cloud resources). 
\item Initially, all resources are allocated to batch
\end{enumerate}

\subsubsection{Configuration of EC2}
Group administrators can sign up for EC2 in egroup by doing a project allocation. In order to do so, they  need to reduce the batch capacity, to free resources for EC2. Next, the group administrator can create group and subgroup allocations for the EC2 access, resulting in a tree like structure. 
Egroups are to be resolved on the CloudMan Frontend.
The OpenNebula backend can then consume the leaves of this tree, which contain lists of users. It will use these user lists to create groups 
in OpenNebula. The resource allocations to these user groups are used to setup group quotas in ONE. 

In the model to be implemented, members of OpenNebula groups will be allowed to see and manage virtual machines of members of the same group they belong to. Thus, group ACLs are setup statically.

\subsubsection{Configuration of Resources}
Moving resources dynamically between the projects requires a unique infrastructure. The current lxcloud setup though uses 2 different hardware 
types, which differ mainly in the size of the local disk space. This adds an additional complication which will provide an excellent test case for
later deployments at CERN, see also the OpenStack backend prototype. 

More experiences are needed in order to find an optimal way to deal with this situation. The different hardware types may be arranged in 
two different zones, one of which may be dedicated to virtual batch only. This way, batch nodes can overflow to the self-service resources, but not
the other way around. Another option is to make use of meta data in CloudMan. It could be used to specify hardware requirements for the two projects.

Moving resources requires a way to drain resources. For virtual batch at CERN this is relatively easy: virtual batch worker nodes have a 
build-in mechanism which will drain them after a while by themselves. The process can be sped up by the backend, if it selects nodes and puts them 
into draining mode. The latter mechanism is more difficult but has a higher potential for portability to a later production instance where batch 
worker nodes will have a long life time. 

Draining the self-service is more tricky. Neither the current OpenNebula nor the current OpenStack installations at CERN provide ways to enforce 
freeing of resources. One way which was discussed to solve this is to introduce QoS classes for resources given in this service. The cheapest 
are machines which can be drained at any time, and live as long as the resources are not needed for higher priorty items. 

\subsubsection{Configuration details}
Setting group quotas in OpenNebula requires a recent version of the software, i.e. ONE 3.6 or above. 




\subsubsection{Further requirements}
While not necessary for the first prototype, which can made work based on the Aldebaran release, future releases should be able to report back the currently available resources to the CloudMan front end, so that it is possible to watch the progress of the migration. 
